---
title: "Experimentation Tools | Sequential Testing"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    css: styles.css
    vertical_layout: scroll
    logo: logo-sm.png
    favicon: favicon.png
    fig_height: 1
    navbar: 
      - { title: "Results Analysis", href: "https://sdidev.shinyapps.io/ABTestAnalysis/" }
      - { title: "Sample Size", href: "https://sdidev.shinyapps.io/sample-size-calculator/" }
      - { title: "Runtime", href: "https://sdidev.shinyapps.io/sample-size-calculator-runtime/" }
      - { title: "Impact Simulation", href: "https://sdidev.shinyapps.io/test-result-simulator/" }
      - { title: "Experimentation ROI", "href:https://sdidev.shinyapps.io/experimentation-roi/" }
runtime: shiny
---


```{r setup, include=FALSE}

library(ggplot2)
library(shiny)
library(gsDesign)
library(dplyr)
library(tidyr)
library(gt)

```

<script>
$('.navbar-logo').wrap('<a href="https://www.searchdiscovery.com/how-we-help/services/optimization/" target=_blank>');
</script>

<div style="display: none;">
```{r url_bookmarking, include = FALSE}
# The code below uses query parameters in the URL of the page so that the total configuration 
# is captured in the URL, enabling someone to "come back" to the exact configuration at any point.
# See details at: https://shiny.rstudio.com/reference/shiny/1.5.0/updateQueryString.html.
# This chunk is wrapped in a <div> that sets the display to none because, otherwise, a little
# bit of JS gets rendered that chunk options are unable to turn off.
# enableBookmarking("url")
# 
# setBookmarkExclude(c("aTraffic", "loadShortcut", "clear", "addResults", "bTraffic", "aConversions", "checkpoint", "bConversions", "shortcut"))
# 
# observe({
# # Trigger this observer every time an input changes
#   reactiveValuesToList(input)
#   session$doBookmark()
# })
# 
# onBookmark(function(state) {
#   state$values$results <- reactiveResultsVars$rslt
# })
# 
# onBookmarked(function(url) {
#   updateQueryString(url)
# })
# 
# onRestore(function(state) {
#   reactiveResultsVars$rslt <- state$values$results
# })
```
</div>

```{r varsAndFunctions, include=FALSE}
# These variables will be used throughout the application
reactiveDesignVars <- reactiveValues(fixedn = NULL, seqn = NULL) # All test DESIGN ouputs
reactiveResultsVars <- reactiveValues(z = NULL, rslt = NULL, win = NULL, tgl = 1) # All RESULTS outputs
reactTalesNum <- reactiveVal() # Turn "tails" radio button output to number from string

# Function can be called to return a sequential design object
createTest <- function(a,b,c,d,e,f,g,h=NULL) {
      alph <- 1 - a/100
      pwr <- b/100
      base <- c/100
      nonf <- d/100
      cvrB <- base * (1 + e/100)
      tls1 <- f
      k_checks <- g
      upBnd <- 3 #upper boundary exponent value (higher is more conservative, typically use 2 or 3)
      lowBnd <- 2 #lower boundary exponent values (higher is more conservative, typically use 2 or 3)
      sides <- if (tls1 > 1) "two.sided" else "one.sided" 
      testType <- if (tls1 > 1) 2 else 4
      cvrA <- if (tls1 > 1) base else base * (1 - nonf)
      
      # FIXED SAMPLE
      n_fixed <- power.prop.test(
        n = NULL,
        p1 = cvrA,
        p2 = cvrB,
        sig.level = alph,
        power = pwr,
        alternative = sides
      )$n
      
      # gsDesign object with evenly spaced checkpoints,
      design <- gsDesign(
        k = k_checks,
        test.type = testType,
        alpha = alph / tls1,
        sfu = sfPower,
        sfupar = upBnd,
        sfl = sfPower,
        sflpar = lowBnd,
        n.fix = n_fixed,
        beta = 1 - pwr
      )
      
      # MAX SAMPLE PER VARIATION
      n_sequential <- tail(design$n.I, 1)


      
      # If updated checkpoints were passed in based on analyses entered, plug them into the analysis here
      if (!is.null(h)) {
        # We'll use these to adjust checkpoints if necessary based on checkpoints till now
        highCheckpt <- 0
        chkptIndex <- 0
        
        # ANALYSIS CHECKPOINTS
        #checkpoints <- list()
        multiplier <- 1/k_checks # % of sample between checkpoints
        checkpoints <- seq(1,k_checks)*multiplier*n_sequential
        
        for (i in 1:length(h)) {
          thisChkpt <- as.numeric(h[[i]][[1]])
          highCheckpt <- if (thisChkpt > highCheckpt) thisChkpt else highCheckpt
          chkptIndex <- if (thisChkpt == highCheckpt) i else chkptIndex
          prevChkpt <- thisChkpt - 1
          chkptN <- as.numeric(h[[i]][[3]])
          
          if (prevChkpt == 0 || chkptN > checkpoints[prevChkpt]) { # if this is the first checkpoint, or if sample size is greater than sample at last checkpoint, then reset the sample size for this checkpoint
            checkpoints[thisChkpt] <- chkptN
          }
        }
        if ((k_checks - highCheckpt) > 1) {
            nNow <- as.numeric(h[[chkptIndex]][[3]]) # get current, latest sample size
            nLeft <- n_sequential - nNow # calculate remaining sample size
            kLeft <- k_checks - highCheckpt # calculate remaining scheduled checkpoints
            mult2 <- 1/kLeft # calculate % of remaining sample size spent at each remaining checkpoint
            chksleft <- seq(1,kLeft)*mult2*nLeft+nNow # calculate sample size at all remaining checkpoints
            highCheckpt <- highCheckpt + 1 # just increment latest checkpoint number by 1
            checkpoints[c(highCheckpt:k_checks)] <- chksleft # Refactor/replace remaining checkpoints
        }
        
        # Generates updated gsDesign object after results have been entered

        finalDesign <-
          gsDesign(
            k = k_checks,
            test.type = testType,
            alpha = alph / tls1,
            beta = 1 - pwr,
            sfu = sfPower,
            sfupar = upBnd,
            sfl = sfPower,
            sflpar = lowBnd,
            n.fix = n_fixed,
            n.I = checkpoints,
            maxn.IPlan = design$n.I[design$k]
          )
      } else {finalDesign <- design}
      


      # Returns the whole test design object (see gsDesign)
      return(finalDesign)

}

```


```{r loadtextshortcut}

# Input to load a configuration from shortcut
div(id = "configLoad",
  h5(class = "ql", "Paste configuration shortcut: "),
  div(class = "ql", textInput("shortcut", NULL, width = "100px")),
  h5(style = "display:inline-block", actionLink("loadShortcut", label = "Load Shortcut")),
  
  # Link to current configuration
  uiOutput('fixedLink')
)
```

```{r quickLoadUtilities, include=FALSE}
# CONFIG SHORTCUT OUTPUT ----
output$fixedLink <- renderUI({
  # Function to put any entered results into a string
  a <- function () {
    b <- ""      
    try ({for (i in 1:length(reactiveResultsVars$rslt)) {
      d <- reactiveResultsVars$rslt[i]
      c <- paste(d[[1]], collapse = "|")
      b <- if (nchar(b) > 1) paste(b,c, sep = ";") else c
    }
    })
    return (b)
  }
  
  # Construct the string
  div(class="fixedLinkRow",
      "Current configuration shortcut:",
  paste(
    input$alpha,
    input$pwr,
    input$cvra,
    input$mde,
    input$tls,
    input$nonf,
    input$traff,
    input$checknum,
    input$conversions,
    input$dayNum,
    input$numVars,
    a(),
    sep = ","
  ))
})
  
# LOAD SHORTCUT ----
observeEvent(input$loadShortcut, {
  # Parse out string, check to make sure it's got all the necessary inputs
  shortcutList <- unlist(strsplit(input$shortcut,","))
  req(length(shortcutList) > 9)
  
  # Update all the inputs with the string values
  updateSliderInput(session, 'alpha', value = shortcutList[1])
  updateSliderInput(session, 'pwr', value = shortcutList[2])
  updateNumericInput(session, 'cvra', value = shortcutList[3])
  updateNumericInput(session, 'mde', value = shortcutList[4])
  updateNumericInput(session, 'tls', value = shortcutList[5])
  updateNumericInput(session, 'nonf', value = shortcutList[6]) 
  updateNumericInput(session, 'traff', value = shortcutList[7])
  updateNumericInput(session, 'checknum', value = shortcutList[8]) 
  updateNumericInput(session, 'conversions', value = shortcutList[9])
  updateNumericInput(session, 'dayNum', value = shortcutList[10])
  updateNumericInput(session, 'numVars', value = shortcutList[11])

  # Update results (behind the scene variables) if present
  if (!is.na(shortcutList[12])) {
    r1 <- strsplit(shortcutList[12],";", fixed = TRUE) # split up the short cut into checkpoints
    r2 <- r1[[1]] # extract
    r3 <- list()
    r4 <- list()
    for (i in 1:length(r2)) { # for number of saved checkpoints
      r3 <- strsplit(r2[i],"|",fixed = TRUE) # split into values
      r3 <- as.numeric(c(r3[[1]])) # grab checkpoint number
      rl <- paste0("checkpoint",r3[1]) # make a label out of checkpoint number
      r4[[rl]] <- r3 # add label to checkpoint results and add to list
    }
    reactiveResultsVars$rslt <- r4
    
  }

})
```

Row {.tabset data-height=370} 
-----------------------------------------------------------------------

### Test Configuration

```{r}
# 2 column screen - Column 1
div(class = "input-two-c",
    h5("Confidence Level"),
    div(sliderInput("alpha",
                 label = NULL, value = 95, min = 50, max = 99, step = 1, round = TRUE)),
    
    h5("Power"),
    div(sliderInput(
      "pwr",
      label = NULL, value = 80, min = 50, max = 99, step = 1, round = TRUE
    )),
    
    h5("Tails"),
    div(
      radioButtons("tls",
             NULL,
             choices = list("1-tail test for superiority" = 1, 
                            "2-tail test for any difference" = 2
                            ),
             selected = 1)
    )

    )

# 2 column screen - Column 2
div(class = "input-two-c",
    h5("Minimum detectable effect"),
    div(
    numericInput(
  "mde",
  label = NULL,
  value = 10,
  min = 0,
  max = 1000,
  step = 1
)
      
    ),

h5("Non-inferiority margin (optional)"),
div(
  numericInput("nonf",
             NULL,
             value = 0,
             min = 0,
             max = 50)
),

h5("Base conversion rate"),
div(
  numericInput(
  "cvra",
  label = NULL,
  value = 10,
  min = 1,
  max = 100,
  step = 1
)
)
)

```


### Current Traffic

```{r}
# 2 column inputs
div(class = "input-two-c",
    h5("Enter current conversion volume to test area"),
    div(numericInput("conversions", NULL, value=2100, step = 100)), 

    h5("Enter current traffic volume to test area"),
    div(numericInput(inputId = "traff",
             label = NULL,
             value = 20000,
             step = 1000)), 

    h5("Calculated base conversion rate"),
    div(class = "big", textOutput("currentCvr", inline = TRUE))
)

div(class = "input-two-c",
    h5("How many days of traffic is this?"),
    div(numericInput("dayNum", NULL, value=28, step = 1)),
    
    h5("Estimated traffic per week"),
    div(class = "big", textOutput("week_traffic", inline = TRUE))
    
    )

```


```{r historical_calculations}
# Output calculated conversion rate from current traffic inputs
output$currentCvr = renderText({paste(round(input$conversions/input$traff*100,2),"%")})

# Output calculated traffic per week
output$week_traffic = renderText({paste(round(input$traff/input$dayNum)*7," visitors")})

```


### Help
##### Confidence level
This is ($1 - \alpha$) where $\alpha$ is your nominal false positive (type-1) error rate when the true effect size is at the null hypothesis threshold.

##### Power
This is ($1 - \beta$) where $\beta$ is your nominal false negative (type-2) error rate. It represents your likelihood of getting a statisticaly significant positive result when the true effect size is the Minimum Detectable Effect.

##### Tails
Tails refer to the outside ends of a cumulative probability distribution and essentially reflect the framing of your null hypothesis. Typically 1-tail is the ideal choice for optimization experiments.

##### Minimum Detectable Effect
a.k.a. Minimum Effect of Interest is the true difference at which Power is guaranteed. So if the true difference in conversion rates is `MDE`, you'll get a true positive `Power`% of the time.

##### Non-inferiority margin
Often, "no practical difference" can be somewhat lower than "0 difference". The non-inferiority margin ($\Delta$) is that margin of indifference. $\alpha$ is guaranteed at $Base ConversionRate(1- \Delta)$.

##### Z-score
Our "test statistic". Statistical significance is derived from a p-value ($1-p$) which in turn is derived from a z-score. The z-score more or less represents the number of standard deviations away from the mean a specific observation is. Here, the mean is control conversion rate and the observation is the test conversion rate.

##### Is a sequential test necessary?
Sequential testing helps you run efficient tests where the effect is unknown and can vary widely from expectations. It can help save you from negative effects impacting customers during a prolonged test as well as it can reduce test duration when you want to react to a large positive difference. That said, it's a more complicated approach to testing and might not be worth your trouble. **Before** committing to a sequential test, check the expected duration with a fixed design. It's healthy to run tests for 2 weeks to capture a few business cycles. If you don't need more than that, consider just sticking with a fixed design.

##### Shortcuts (saving your work)
Above the input panel, there is a place to copy a shortcut to the current configuration including any results entered to date, or to load a shortcut from a past configuration. We know, a url would be ideal. Alas! Free software! 

##### 1-tail vs 2-tail
Selecting 1-tail for a 'superiority test' will produce a **futility boundary**, allowing you to end the test early when test results are poor and a comeback is not likely while preserving power. Selecting 2-tail will produce identical (symmetrical) upper and lower decision boundaries.

##### Using the Power Plot
Use the power plot to help you plan your test configuration. The probabilities of false positives and false negatives at different effect sizes can prove very informative. Try adding a **non-inferiority margin** and see what happens!

##### Timing adjustments
By default, checkpoints are planned at even intervals. However, should your actual checkpoints differ, **boundaries and checkpoints will automatically adjust** whenever you enter results. 
    * To be explicit, you are not bound to the *timing* of the check-ins, only to the number of check-ins.

##### Early check-ins
You might want to **plan your first check for early** in the test. This could help identify problematic test experiences and prevent loss.

##### Non-binding decision boundary
The **lower** decision boundary for a 1-tail test is what's known as a **non-binding** boundary. This means, when you cross it, you can still choose to continue the test while preserving your type-1 error rate and only improving your power. 

##### Binding decision boundary
The **upper** decision boundary in a 1-tail test and both boundaries in a 2-tail test are **binding** boundaries. When your results cross them, you must end the test in order to preserve your type-1 error rate.

##### P-value adjustments
Standard Confidence Interval and p-value calculations will be understated and cannot be taken at face value. **We provide adjusted p-values and confidence intervals** when the upper (efficacy) boundary is crossed.

##### Why use z-scores?
You are likely more accustomed to seeing p-values, which are derived from the z-score. But we don't show the p-value because it cannot be interpretted as it normally would be in a fixed-horizon test. In other words, we're trying to avoid misinterpretation and statistical faux pas.


### Multiple Comparisons
```{r}
# TO BE DEVELOPED
#
# Boundary type
#
# Test Type
# selectInput("testType", 
#             label = NULL,
#             choices = c())
#
# Upper boundary sensitivity
# Lower boundary sensitivity
# O’Brien-Fleming, Pocock, or Wang-Tsiatis are normally used with equally-spaced analyses. They
# are used only with one-sided (test.type=1) and symmetric two-sided (test.type=2) designs. We
# will use the CAPTURE example, again with 80% power rather than the default of 90%. Notice
# that this requires specifying beta=.2 in both nBinomial() and gsDesign(). O’Brien-Fleming,
# Pocock, or Wang-Tsiatis (parameter of 0.15) bounds for equally space analyses are generated using
# the parameters sfu and sfupar below. I

div(class = "input-two-c multiple",
    h4("Warning: Advanced!"),
    #br(),
    p("The Results Analysis panel will be rendered inaccurate in the case of multiple testing (calculating p-values for multiple test variations or metrics). We use the ",
      tags$a(href="https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method", "Holm method"), 
      "to make adjustments to the z-score boundary for each comparison. Only the upper boundary is given which means that for 1-tail, there will be no futility boundary and for 2-tail you just switch the sign when the difference is negative."),
    
    h5("How many additional variants or metrics are you testing?"),
    div(numericInput("numVars",
                 label = NULL, value = 0, min = 1, max = 20, step = 1))
)

div(class = "input-two-c large",
    p("When analyzing, you must rank z-scores high to low by their absolute values and apply the new z-score boundaries provided in the table. 'Z-1' applies to the highest z-score and so on."),
    gt_output("adjBounds")
)

```

```{r multipleComparisons, include=FALSE}

output$adjBounds <- render_gt({
  req(input$numVars > 0)
  req(!is.null(reactiveDesignVars$table$complete$upper))
  
  df <- data.frame(
    checkpoints = paste("Checkpoint",seq(1,input$checknum)),
    maxSample = reactiveDesignVars$table$complete$samples / 2,
    origZscors = reactiveDesignVars$table$complete$upper) 

  comps <- input$numVars + 1
  
  for (i in seq(1:comps)) {

    rank <- paste0("Z-",i)
    
    pVals <- 1-pnorm(reactiveDesignVars$table$complete$upper)
    pVals <- pVals/(comps-i+1)
    zVals <-  abs(qnorm(pVals))
    
    df[rank] <- zVals
    
 
  }
  
  gt(df) %>%
    fmt_number(columns = contains("z"), decimals = 2) %>%
    fmt_number(columns = contains("sample"), decimals = 0) %>%
    cols_width(contains("check") ~ px(90),
               contains("sample") ~ px(80),
               contains("zscor") ~ px(70)
               ) %>% 
    cols_label(checkpoints = "",
               maxSample = "(Max) Sample Size Per Variant",
               origZscors = "Old Boundary") %>%
    tab_options(
      container.height = 275,
      container.overflow.x = TRUE,
      container.overflow.y = TRUE)
})

output$multiwarn <- renderUI({
  if (input$numVars > 0) {
  div(id="multiplewarning",
      "Looks like you have configured the test to run with multiple variants or metrics to compare. Many features will not display due to this.")
  }
})

```


Row {data-height=150}  
-----------------------------

### Planned analyses (checkpoints)

##### How many times do you want to "peek" and analyze test results? (max 18)

```{r}
div(
div(style="display:inline-block;", numericInput(
  "checknum", label = NULL, min = 2, max = 18, value = 4, step = 1, width = 100
)),
div(style="display:inline-block; padding-left: 5px;", "(Including final analysis)"))
```


```{r createAndStoreDesign, include=FALSE}
# CREATE SEQUENTIAL TEST DESIGN
# Listen for changes in any inputs or submitted results
observeEvent(c(input$alpha, input$pwr, input$cvra, input$nonf, input$mde, input$tls, input$checknum, reactiveResultsVars$tgl),{ #}, reactiveResultsVars$rslt),{
  req(input$cvra > 0)
  req(input$mde > 0)
  req(input$nonf >= 0)
  req(input$checknum >= 2)
  
  reactiveDesignVars$tls <- as.numeric(input$tls)
  
  tryCatch({
    
    # If there are results, it's a difference function call (boundaries are adjusted)
    if (is.null(reactiveResultsVars$rslt)) {
      testDesign <- createTest(input$alpha, input$pwr, input$cvra, input$nonf, input$mde, reactiveDesignVars$tls, input$checknum)
    } else  {
      testDesign <- createTest(input$alpha, input$pwr, input$cvra, input$nonf, input$mde, reactiveDesignVars$tls, input$checknum, reactiveResultsVars$rslt)
    }
  
  
  # Take all the design outputs and assign to individual variables used throughout app
  reactiveDesignVars$fixedn <- testDesign$n.fix*2
  reactiveDesignVars$seqn <- tail(testDesign$n.I,1)*2
  reactiveDesignVars$fulldesign <- testDesign
  reactiveDesignVars$table$timing <- testDesign$timing
  reactiveDesignVars$table$samples <- testDesign$n.I * 2
  reactiveDesignVars$table$lower <- testDesign$lower$bound
  reactiveDesignVars$table$upper <- testDesign$upper$bound
  
  # Sequence of deltas based on MDE
  m <- input$mde
  n <- -input$nonf
  negD <- -m+n
  posD <- m-n
  
  delta_list <- c()
  theta_list <- c()
  
  if (input$tls == 2) {
    delta_list <- c(negD*2.5, negD*2.25, negD*2, negD*1.75, negD*1.5, negD*1.25)
    theta_list <- seq(-2.5,-1.25,.25)
  }
  
  delta_list <- c(delta_list,negD*1+n,negD*.75+n,negD*.5+n,negD*.25+n,n,posD*.25+n,posD*.5+n,posD*.75+n,m,posD*1.25+n,posD*1.5+n,posD*1.75+n,posD*2+n,posD*2.25+n,posD*2.5+n)
  
  reactiveDesignVars$deltas <- delta_list
  theta_list <- c(theta_list,seq(-1,2.5,.25))
  
  # reactiveDesignVars$deltas <- c(negD*1+n,negD*.75+n,negD*.5+n,negD*.25+n,n,posD*.25+n,posD*.5+n,posD*.75+n,m,posD*1.25+n,posD*1.5+n,posD*1.75+n,posD*2+n,posD*2.25+n,posD*2.5+n)
  # reactiveDesignVars$deltas <- seq(-1,2.5,.25)*(input$mde+input$nonf)
  thetas <- theta_list*testDesign$delta
  reactiveDesignVars$probOb <- gsProbability(d = testDesign, theta = thetas)

  # DF for results outputs
  reactiveDesignVars$table$complete <- data.frame(
    times = testDesign$timing,
    samples = testDesign$n.I * 2,
    days = testDesign$n.I * 2 / (input$traff/input$dayNum),
    lower = testDesign$lower$bound,
    upper = testDesign$upper$bound,
    results = 0
  )
  

  # Evaluate any results entered
  if (length(reactiveResultsVars$rslt) > 0) {
    
    # Add any z-scores in results column of design table
    for (i in 1:length(reactiveResultsVars$rslt)) {
      currRslt <- reactiveResultsVars$rslt[i]
      currChk <- currRslt[[1]][1]
      currZ <- currRslt[[1]][6]
      reactiveDesignVars$table$complete[currChk, 6] = currZ
    }
    
    # Evaluate results outcomes
    tls_text <- if (reactiveDesignVars$tls == 1) "_1tail" else "_2tail" 
    for (i in 1:length(reactiveResultsVars$rslt)) {
      rslt <- reactiveResultsVars$rslt[i]
      chk <- as.numeric(rslt[[1]][1])
      complete_text <- if (chk == input$checknum) "complete" else "early"
      zscr <- rslt[[1]][6]
      boundary <- if (zscr > testDesign$upper$bound[chk]) "_upper" else if (zscr < testDesign$lower$bound[chk]) "_lower" else "_middle"
      reactiveResultsVars$status <- if (boundary == "_middle") paste0(complete_text,boundary) else paste0(complete_text,boundary,tls_text)
      reactiveResultsVars$win$outcome <- if (boundary != "_middle")
      
      if (zscr > testDesign$upper$bound[chk]) {
        reactiveResultsVars$win <- list(checkpoint = chk, outcome = "efficacy")
        break
      } else if (zscr < testDesign$lower$bound[chk] && input$tls == 2) {
        reactiveResultsVars$win <- list(checkpoint = chk, outcome = "inefficacy")
        break
      } else if (zscr < testDesign$lower$bound[chk] && input$tls == 1) {
        reactiveResultsVars$win <- list(checkpoint = chk, outcome = "futility")
      } else {
        reactiveResultsVars$win <- list(checkpoint = chk, outcome = NULL)
      }
    }
    # "complete_upper",
    # "complete_lower_2tail",
    # "complete_lower_1tail",
    # "complete_middle",
    # "early_upper",
    # "early_lower_1tail",
    # "early_lower_2tail",
    # "early_middle" 
  }
  }, error = function(e) {
    showNotification("Whoops! How embarrassing. Seems you triggered an error. Pleast consider reporting the bug using the link at the bottom of this document.", type="error", duration = 5)
  })
})


```

### Sample size comparison
```{r sampleOutputs, include=FALSE}
output$fixedN <- renderText({
  # If there are multiple comparisons, we'll report in terms of per variation sample size since we can't assume the comparisons are separate variants.
  if (input$numVars < 1) {
    result <- format(round(reactiveDesignVars$fixedn), big.mark=",", scientific = FALSE)
  } else {
    val <- reactiveDesignVars$fixedn / 2
    result <- format(round(val), big.mark=",", scientific = FALSE)
    result <- paste(result,"per variant")
  }
  
  result
  
  })

output$fixedDays <- renderText({
  # If there are multiple comparisons, we'll report in terms of per variation days since we can't assume the comparisons are separate variants.
  if (input$numVars < 1) round(reactiveDesignVars$fixedn/(input$traff/input$dayNum)) else round(reactiveDesignVars$fixedn/2/(input$traff/input$dayNum))
  
  })

output$seqN <- renderText({
  # If there are multiple comparisons, we'll report in terms of per variation sample size since we can't assume the comparisons are separate variants.
  if (input$numVars < 1) {
    result <- format(round(reactiveDesignVars$seqn), big.mark=",", scientific = FALSE)
  } else {
    val <- reactiveDesignVars$seqn / 2
    result <- format(round(val), big.mark=",", scientific = FALSE)
    result <- paste(result,"per variant")
  }
  
  result
  
  })

output$maxDays <- renderText({
# If there are multiple comparisons, we'll report in terms of per variation days since we can't assume the comparisons are separate variants.
  if (input$numVars < 1) round(reactiveDesignVars$seqn/(input$traff/input$dayNum)) else round(reactiveDesignVars$seqn/2/(input$traff/input$dayNum))  })

output$diff <- renderText({
  paste0(round((reactiveDesignVars$seqn/reactiveDesignVars$fixedn-1)*100),"%")
  })
```


```{r}
div(
  h5("Fixed-horizon sample size: ",
     strong(textOutput("fixedN", inline = TRUE)),
     " (Estimated ",
     strong(textOutput("fixedDays", inline = TRUE)),
     " days)")
)

div(
  h5("Sequential test maximum sample size: ",
     strong(textOutput("seqN", inline = TRUE)),
     " (Estimated ",
     strong(textOutput("maxDays", inline = TRUE)),
     " days)")
)

div(
  h5("That's a maximum increase of ",
     strong(textOutput("diff", inline = TRUE)),
     " See the Expected Sample Size chart below to see what test duration is expected to be based on different effect sizes.")
)

```


Row {data-height=450}
-----------------------------
### Power analysis
This chart plots the likelihood of crossing a decision boundary at each checkpoint for different effect sizes.

```{r powerPlot}

# Configure the power plot
output$pwrPlot <- renderPlot({
  req(input$numVars == 0)

  probOb <- reactiveDesignVars$probOb
  deltas <- reactiveDesignVars$deltas/100
  
  upProb <- data.frame(probOb$upper$prob) %>%
    mutate(cumsum(.)) %>%
    t() %>%
    data.frame()

  
  colnames(upProb) <- paste0("Upper_Checkpoint",seq(1,ncol(upProb)))
  upProb <- mutate(upProb, "Effects" = deltas)

  loProb <- data.frame(probOb$lower$prob) %>%
    mutate(cumsum(.)) %>%
    t() %>%
    data.frame()

  colnames(loProb) <- paste0("Lower_Checkpoint",seq(1,ncol(loProb)))
  loProb <- mutate(loProb, "Effects" = deltas)

  
  df <- merge(upProb,loProb) %>%
    pivot_longer(cols = !contains("Effects"), names_to = c("Boundary","Checkpoint"), names_pattern = "(.*)_Checkpoint(.*)", values_to = "Probability") 
  
  ggplot(df, aes(x=Effects, y=Probability, color=Boundary, linetype=Checkpoint)) +
    #geom_line() +
    geom_smooth(method="gam", se=FALSE, fullrange=FALSE, size=.75) +
    theme_light() +
    scale_y_continuous(labels = scales::percent_format(accuracy=1), n.breaks = 10) +
    scale_x_continuous(labels = scales::percent, n.breaks = 8)
  
}, height = 350)

```

```{r}
div(plotOutput("pwrPlot"), 
    uiOutput("multiwarn")
    )
```

### Expected sample size by effect size
This chart displays the expected sample size by the actual effect size of the treatment.

```{r expectedSamplePlot}
# Configure the expected sample size plot
output$samplePlot <- renderPlot({
  #req(as.numeric(input$tls) < 2)
  req(input$numVars == 0)

  
  nList <- reactiveDesignVars$table$samples
  probOb <- reactiveDesignVars$probOb
  deltas <- reactiveDesignVars$deltas/100
  nperDay <- round(input$traff/input$dayNum) 
  
  # Combine probabilites of crossing upper and lower boundaries for different effect sizes
  combo_probs <- probOb$upper$prob+probOb$lower$prob 

  combo_probs <- head(combo_probs,-1) # Drop last checkpoint 
  last_row <- 1-colSums(combo_probs)  # New vector for last checkpoint taking 1 - sum of prior checkpoints
  combo_probs <- combo_probs %>% 
    rbind(last_row)    # Put them together
  
  # Multiply probabilities by sample sizes at checkpoints
  combo_probs <- round(combo_probs * nList) %>% 
    t() %>%
    data.frame() %>%
    mutate(expLength = rowSums(.)) %>%
    mutate(effects = deltas)

  # upProb <- data.frame(probOb$upper$prob*nList) %>%
  #  t() %>%
  #  data.frame() %>%
  #  mutate(expLength = rowSums(.)) %>%
  #  mutate(effects = deltas) 
  # 
  # loProb <- data.frame(probOb$lower$prob*nList) %>%
  #  t() %>%
  #  data.frame() %>%
  #  mutate(expLength = rowSums(.)) %>%
  #  mutate(effects = deltas)
  # 
  # df <- data.frame(Effects = deltas, ExpSample = upProb$expLength+loProb$expLength)

  # Make df of expected sample size by effect
  df <- data.frame(Effects = deltas, ExpSample = combo_probs$expLength)

  spline_df <- as.data.frame(spline(df$Effects, df$ExpSample))
 
  ggplot(df, aes(x = Effects, y = ExpSample)) +
    geom_line(data = spline_df, aes(x = x, y = y), color = "cyan3") +
    geom_hline(yintercept = reactiveDesignVars$fixedn, size=.25, linetype = "dashed") +
    annotate(geom = "text", x = 0,
         y = reactiveDesignVars$fixedn * .99, 
         label= "Fixed horizon sample size",
         fontface = "bold",
         size = 4.0) +
    labs(y="Expected Sample Size") +
    theme_light() +
    scale_x_continuous(labels = scales::percent, n.breaks = 8) + 
    scale_y_continuous(sec.axis = sec_axis(~ ./nperDay, name = "Estimated Duration (Days)"), labels = scales::comma)
  
}, height = 350)

```


```{r}
plotOutput("samplePlot")
```

Row  
-----------------------------

### Enter results

```{r varA_results_inputs}
h3("Control Experience")
div(class='three-c',
  h5('Conversions'),
  div(numericInput("aConversions", label = NULL, value = "200", step = 10)
      )
)

div(class='three-c',
  h5('Visitors'),
  div(numericInput("aTraffic", label = NULL, value = "3000", step = 100)
      )
    )

div(class='three-c',
  h5('Conversion rate'),
  div(class = "big", textOutput("aRate", inline = TRUE)
      )
)

```

```{r varB_results_inputs}
h3("Test Experience")
div(class='three-c',
  h5('Conversions'),
  div(numericInput("bConversions", label = NULL, value = "270", step = 10)
      )
)

div(class='three-c',
  h5('Visitors'),
  div(numericInput("bTraffic", label = NULL, value = "3000", step = 100)
      )
    )

div(class='three-c',
  h5('Conversion rate'),
  div(class = "big", textOutput("bRate", inline = TRUE)
      )
)

```


```{r zscore_display}
h3("Calculations")
div(class = 'three-c',
    h5('Difference'),
    div(class = "big", textOutput("rateDiff")))

div(class = 'three-c',
    h5('Z-score'),
    div(class = "big", textOutput("zscore")))

div(class = 'three-c',
    h5('% of Sample size'),
    div(class = "big", textOutput("proportion")))
    
```

```{r checkpoint_selection_input}
#h3("Checkpoint selection")
h5(style="margin:10px 10px 0px 10px;",
   "Which analysis do these results apply to?")
div(class = "chkpt-selector",
    div(id="checkselectipt", htmlOutput("checkSelect")),
    div(id="checkselectbtn", actionLink("addResults", "Add to results")),
    div(id="checkselecttxt", "(added to the selected checkpoint)")
        )
```

```{r results_outputs, include=FALSE}
# Configure results outputs except for plots -- those are in their corresponding section

# Checkpoint selector output
output$checkSelect <- renderUI({
  selectInput("checkpoint",
              NULL,
              choices = seq(1,input$checknum),
              selected = 1)
})

# Conversion rate calculation outputs
output$aRate <- renderText({paste0(round(input$aConversions / input$aTraffic * 100,2), "%")})
output$bRate <- renderText({paste0(round(input$bConversions / input$bTraffic * 100,2), "%")})

# Conversion difference
output$rateDiff <- renderText({
  req(input$aConversions > 1)
  req(input$bConversions > 1)
  req(input$aTraffic > 10)
  req(input$bTraffic > 10)
  
  diff <-
    round(((input$bConversions / input$bTraffic) / (input$aConversions / input$aTraffic) -
             1
    ) * 100, 2)
  paste0(diff, "%")
})

# Updates zscore calculation when results inputs change
observeEvent(c(input$aConversions, input$bConversions, input$aTraffic, input$bTraffic),{
  req(input$aConversions > 1)
  req(input$bConversions > 1)
  req(input$aTraffic > input$aConversions)
  req(input$bTraffic > input$bConversions)
  
  a <- input$aConversions
  b <- input$bConversions
  c <- input$aTraffic
  d <- input$bTraffic
  e <- if (input$nonf > 0) input$nonf/100 else 0
  
  # Calculate a z-score
  reactiveResultsVars$z <- round(-testBinomial(x1=a, x2=b, n1=c, n2=d, delta0 = a/c*e), digits = 2)
  
})

# Zscore output
output$zscore <- renderText({
  req(reactiveResultsVars$z != 0)
  reactiveResultsVars$z})

# Pct to sample output
output$proportion <- renderText({
  req(input$aConversions > 1)
  req(input$bConversions > 1)
  req(input$aTraffic > 10)
  req(input$bTraffic > 10)
  
  nprop <- (round(100*(input$aTraffic + input$bTraffic)/reactiveDesignVars$seqn))
  
  paste0(nprop,"%")
})

# CTA action adds current inputs to reactive results list
observeEvent(input$addResults, {
  inchk <- as.numeric(input$checkpoint)
  rownum <- paste0("checkpoint",inchk)
  zadd <- reactiveResultsVars$z
  rsltList <- c(inchk,input$aConversions,input$aTraffic,input$bConversions,input$bTraffic,zadd)
  
  reactiveResultsVars$rslt[[rownum]] <- rsltList
  reactiveResultsVars$tgl <- reactiveResultsVars$tgl * -1

}, ignoreInit = TRUE)

# When changing the checkpoint selected, if there are stored results, populate inputs with them
observeEvent(input$checkpoint, {
  try({
  for (i in 1:length(reactiveResultsVars$rslt)) {
      d <- reactiveResultsVars$rslt[i]
      a <- d[[1]][1]
      if (!is.null(a) && a == as.numeric(input$checkpoint)) {
        updateNumericInput(session,"aConversions", value = d[[1]][2])
        updateNumericInput(session,"aTraffic", value = d[[1]][3])
        updateNumericInput(session,"bConversions", value = d[[1]][4])
        updateNumericInput(session,"bTraffic", value = d[[1]][5])
      }
  }
  })
}, ignoreInit = TRUE)

```


### Recorded Test Data

The table below displays the decision boundaries in terms of z-scores for each checkpoint along with the z-scores of any results recorded to date.

```{r results_table_calc, include=FALSE}

output$resultTable <- render_gt({
  req(input$numVars == 0)
  gt(reactiveDesignVars$table$complete) %>%
    cols_label(times = "% Complete",
               samples = "Total Sample",
               days = "Est. Days",
               lower = "Lower",
               upper = "Upper",
               results = "Outcomes") %>%
    fmt_percent(columns = times, decimals = 0) %>%
    fmt_number(columns = c(samples, days), decimals = 0) %>%
    fmt_number(columns = c(lower,upper,results), decimals = 2) %>%
    tab_spanner(columns = c(lower,upper), label = "Decision Boundaries")

  
  }) 

```

```{r clearResults_calc, include=FALSE}
# Clear all the reactive results variables (list)
observeEvent(input$clear, {
  req(!is.null(reactiveResultsVars$rslt))
  
  reactiveResultsVars$rslt <- NULL
  reactiveResultsVars$win <- NULL
  reactiveResultsVars$status <- NULL
  reactiveDesignVars$table$complete$results <- 0
}, ignoreInit = TRUE)
```


```{r results_table}
# Table
div(class = "ztable", gt_output("resultTable"))

# Confidence and Interval calculations only show if there is conclusive outcome
div(class = "confidence",
br(),
# Clear results link
div(actionLink("clear","Clear all results"))
)



```


Row {data-height=400}
-----------------------------
### Test Outcome

``` {r test_outcome_observer, include = FALSE}

observeEvent(reactiveResultsVars$win$outcome, {
  req(!is.null(reactiveResultsVars$win))
  
  # Set all the variables needed
  checkNum <- paste0("checkpoint",reactiveResultsVars$win$checkpoint)
  checkNumInt <- reactiveResultsVars$win$checkpoint
  nList <- reactiveDesignVars$table$samples/2
  winZ <- reactiveResultsVars$rslt[[checkNum]][6]
  Ns <- if (checkNumInt == 1) nList[1] else c(nList[1:checkNumInt])# Ns is list of sample sizes at check-ins, these are for 1 variation, not total
  Zs <- if (checkNumInt == 1) winZ else c(reactiveDesignVars$table$upper[1:(checkNumInt-1)],winZ) # upper boundary z-scores from test design up to most recent check-in, original z-scores for all but the most recent which is calculated z-score 
  xA <- reactiveResultsVars$rslt[[checkNum]][2] # number of conversions in control
  xB <- reactiveResultsVars$rslt[[checkNum]][4] # number of conversions in test
  nA <- reactiveResultsVars$rslt[[checkNum]][3] # number of samples in control
  nB <- reactiveResultsVars$rslt[[checkNum]][5] # number of samples in test
  ciZ <- reactiveDesignVars$table$upper[checkNumInt] # upper boundary z-score from original design for the most recent check-in point
  tls <- reactiveDesignVars$tls # tls number of tails
  cvra <- xA/nA
  cvrb <- xB/nB
  
  # Adjusted p-value once a critical boundary has been crossed
  probs <- gsProbability(k=length(Ns), theta=0, n.I=Ns, a=array(-20,length(Ns)),b=Zs)
  pval <- sum(probs$upper$prob) * tls 
  conf <- abs(round((1-pval)*100, digits=4))
  
  # Compute adjusted CI 
  ci <- ciBinomial(x1=xB, x2=xA, n1=nB, n2=nA,alpha=2*(1-pnorm(ciZ))) 
  ci <- ci/cvra
  
  # Add the observed effect
  ci$mean <- cvrb/cvra-1
  
  # Make the text links
  textConf <- if (reactiveResultsVars$win$outcome == "efficacy" || reactiveResultsVars$win$outcome == "inefficacy" ) paste0(conf,'%') else NULL
  
  # Set the reactive variables on which the outputs rely
  reactiveResultsVars$win$conf <- textConf 
  reactiveResultsVars$win$ciData <- ci

}, ignoreInit = TRUE)

```

``` {r test_outcome_outputs, include = FALSE}
# Outcome message
# output$outcome <- renderText({
#   req(!is.null(reactiveResultsVars$win))
#   
#   if (reactiveResultsVars$win$outcome == "efficacy") {
#     return("The test experience has passed the efficacy boundary.")
#   } else if (reactiveResultsVars$win$outcome == "futility") {
#     return("The test experience has passed the futility boundary.")
#   } else if (reactiveResultsVars$win$outcome == "inefficacy") {
#     return("The test experience has passed the lower decision boundary.")
#   }
# })


# Default messaging
output$results_default <- renderText({
  req(is.null(reactiveResultsVars$win$ciData))
  print(reactiveResultsVars$win$ciData)
  
  "Once test results have been recorded and a decision boundary has been crossed and/or the max sample size has been reached, an outcome will appear here."
  
})

# Confidence interval plot output definition
output$ci_plot <- renderPlot({
  req(!is.null(reactiveResultsVars$win$ciData))
  
  ci <- reactiveResultsVars$win$ciData
  
  ci_labels <- c(paste0(round(ci$lower*100,2),"%"),paste0(round(ci$mean*100,2),"%"),paste0(round(ci$upper*100,2),"%"))
  stat_sig_text <- if (!is.null(reactiveResultsVars$win$conf[[1]]))  paste0("Statistically significant at ", reactiveResultsVars$win$conf[[1]], " confidence") else " "
  
  ggplot(ci, aes(x=mean, y=1)) +
    annotate("text",
             x=ci$mean,
             y=1.8,
             label="Confidence interval of effect (B/A-1)",
             fontface = "bold",
             size = 5) +
    annotate("text",
             x=c(ci$lower,ci$mean,ci$upper),
             y=1.45,
             label=c("Lower","Observed","Upper"),
             fontface = "bold",
             size = 4) +
    annotate("text",
             x=c(ci$lower,ci$mean,ci$upper),
             y=1.2,
             label=ci_labels) +
    geom_point() +
    geom_errorbarh(aes(xmin=lower, xmax=upper, height=.1)) +
    annotate("text",
             x=ci$mean,
             y=.5,
             label=stat_sig_text,
             size=4,
             color="#01bfc4") +
    geom_vline(xintercept = 0,
               linetype = "dashed",
               size = .25,
               color = "#666666") +
    annotate("text", x=0, y =.4, label="No effect", size = 3, angle = 90) +
    scale_y_continuous(limits = c(0,2)) +
    theme_minimal() +
    theme(axis.line = element_blank(), axis.text = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), aspect.ratio = .3)
    
}, height = 150)

# Action advice message
output$advice <- renderText({
  req(!is.null(reactiveResultsVars$status))
  
  message <- switch(reactiveResultsVars$status,
         "complete_upper" = "Maximum sample size has been reached. You should end the test and reject the null hypothesis. Congratulations on the win!",
         "complete_lower_2tail" = "Maximum sample size has been reached. You should end the testand reject the null hypothesis. We hope it's a win!",
         "complete_lower_1tail" = "Maximum sample size has been reached. You should end the test. Unfortunately, the results are inconclusive and you cannot reject the null hypothesis.",
         "complete_middle" = "Maximum sample size has been reached. You should end the test. Unfortunately, the results are inconclusive and you cannot reject the null hypothesis.",
         "early_upper" = "The test has crossed the upper decision boundary early. You should end the test and reject the null hypothesis. Congratulations on the win!",
         "early_lower_1tail" = "The test has crossed the lower decision boundary early. In this case, you can end the test (80% power will be preserved on your MDE) or you can opt to continue the test which will serve to increase your power.",
         "early_lower_2tail" = "The test has crossed the lower decision boundary early. You should end the test and reject the null hypothesis. We hope it's a win!",
         "early_middle" = "The test has yet to cross a decision boundary. You should continue testing in order to achieve the planned statistical power."
         )
  
  message
})
```


```{r test_outcome_display}
# Default, no-results message
div(class = "results-default",
  div(
    textOutput("results_default")
  )
)

# Confidence and Interval calculations only show if there is conclusive outcome
div(class = "confidence",
  div(class = "big",
    textOutput("outcome")
  )
)

# Confidence interval plot display
div(class = "ci-plot",
    plotOutput("ci_plot")
    )

# Advice message
div(class = "advice",
    textOutput("advice")
)

```

### Decision Boundary Chart
As results are recorded, the calculated test statistic (z-score) will appear on this chart. The actionable significance threshold will change depending on the checkpoint and sample size, with the bar lowering throughout the test. In other words, results will have to more extreme to end a test early.

```{r boundary_plot_calc, include=FALSE}
output$boundPlot <- renderPlot({
  req(input$numVars == 0)

  df <- reactiveDesignVars$table$complete
  df$results[df$results == 0] <- NA 
  nperDay <- round(input$traff/input$dayNum)

  ggplot(df, aes(x = samples)) +
    geom_line(mapping = aes(y=upper), color = "cyan3") +
    geom_line(mapping = aes(y=lower), color = "brown1") +
    geom_line(mapping = aes(y=results), color = "chartreuse3", linetype = 2) +
    geom_ribbon(mapping = aes(ymin = min(lower), ymax = lower), fill = "brown1", alpha = .5) +
    geom_ribbon(mapping = aes(ymin = upper, ymax = max(upper)), fill = "cyan3", alpha = .5) +
    geom_vline(xintercept = reactiveDesignVars$fixedn, size=.25, linetype = "dashed") +
    annotate(geom = "text", y = 1,
         x = reactiveDesignVars$fixedn * 1.02, 
         label= "Fixed horizon sample size",
         fontface = "bold",
         angle = 90,
         size = 4.0) +
    labs(y="Expected Sample Size") +
    theme_light() +
    labs(x="Sample Size", y="Z-score") +
    scale_x_continuous(sec.axis = sec_axis(~./nperDay, name = "Estimated Duration (Days)"), labels = scales::comma) +
    scale_y_continuous(n.breaks = 8) +
    geom_label(y=df$upper, label = round(df$upper,2)) +
    geom_label(y=df$lower, label = round(df$lower,2)) +
    geom_label(y=df$results, label = round(df$results,2), color = "chartreuse3")
    
}, height = 350)

```

```{r boundary_plot_output}
# Boundary Plot
div(class = "bounds",
plotOutput("boundPlot")
    )
```


Row {data-height=30}
-----------------------------
v3.0 - Thanks for using! For version history, to report bugs and submit feature requests [click here](https://github.com/alphanumerritt/sequential-test-app/issues){target="_blank"}.

Row {data-height=80}
-----------------------------

```{r technotes}
p(
  class = 'postS',
  strong("Technical Bits"),
  " - This application uses the R package",
  tags$a(href = "https://cran.r-project.org/package=gsDesign", target =
           "_blank",
         "gsDesign"),
  " to compute the sequential test sample sizes and decision boundaries.",
  "You can find some educational documentation on the methods",
  "(not just help modules)",
  tags$a(href = "https://r-forge.r-project.org/scm/viewvc.php/*checkout*/pkg/gsDesign/inst/doc/gsDesignManual.pdf?revision=382&root=gsdesign", target =
           "_blank",
         "here"),
  ". There are, in fact, many ways to formulate and customize a sequential test with decision boundaries.",
  " The approaches differ in terms of the shape of the boundary curve (or lack thereof) and how ",
  " conservative the approach to alpha and beta spending. For 1-tail tests, we use test.type=4, sfu=sfPower, sfupar=3, and sflpar=2 ",
  "as parameters for the design function. These specify a Kim-Dements power spending function with a non-binding lower boundary ",
  "and a binding upper boundary, as well as a more conservative (harder to pass)",
  "early upper boundary than lower boundary. For 2-tail tests, we use test.type=2 instead which makes both boundaries binding."
)

```

Row {data-height=100}
-----------------------------
```{r credits}
p(
  class = 'postS',
  strong("Credits"),
  'This application was only made possible by the work and contributions of so many others. ',
  'Many academic adventurers did the real work on developing the statistical methods we all rely on. ',
  'Pocock, O\'Brien, Flemming, Lan, Kim, Demets, Jennison, Turnbill and many others blazed the trails. ',
  br(),
  'Georgi Georgiev synthesized the academics\' work and appropriated it for online a/b testing. ',
  'His body of literature over at analytics-toolkit.com, provides an excellent introduction to the statistics and a guidebook on how to use them. ',
  'We based the alpha spending functions in our calculator on those documented in his AGILE method. ',
  'Also, his suite of calculators is excellent.',
  'In fact, it\'s worth noting that Georgiev\'s treatment of bias in confidence calculations is preferrable ',
  'to the method offered in the gsDesign R package which we use here.',
  br(),
  'Speaking of gsDesign, Keaven Anderson of Merck Research Laboratories did an amazing job crafting and ',
  'documenting this package. Our application is little more than a highly restrictive GUI for it. '
)
```

